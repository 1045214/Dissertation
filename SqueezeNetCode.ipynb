{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUK4+NvDD+IEO0igaCPIZP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mhobo/Dissertation/blob/main/SqueezeNetCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp72EnKY_8y3",
        "outputId": "a61b6f8a-332e-41cf-ce2f-a3b086bdef11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "import random as rnd\n",
        "from scipy.stats import bernoulli\n",
        "import time\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "batch_size = 125\n",
        "num_workers = 2\n",
        "\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='data/',\n",
        "                                             train=True, \n",
        "                                             transform=transforms.ToTensor(),  #replace with = transform after\n",
        "                                             download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='data/',\n",
        "                                            train=False,#need comma here to get transforms back\n",
        "                                            transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                            batch_size= batch_size, \n",
        "                                            shuffle=True, num_workers = num_workers)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                            batch_size= batch_size, \n",
        "                                            shuffle=False, num_workers = num_workers)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Corrupts Labels\n",
        "def Changer(p = 0.5):\n",
        "\n",
        "#  Changed = range(0,6000,Ratio)\n",
        "  Changed_dataset = copy.deepcopy(train_dataset)\n",
        "# set new labels that are wrong\n",
        "  New_Values = []\n",
        "  \n",
        "  for i in range(len(Changed_dataset.targets)): #[Changed]\n",
        "    if bernoulli.rvs(p) == 1:\n",
        "      continue\n",
        "    else:\n",
        "\n",
        "      Label_Options = list(range(0,10))\n",
        "      Label_Options.remove(Changed_dataset.targets[i]) #Changed[i]\n",
        "      New_Value = rnd.choice(Label_Options)\n",
        "      Changed_dataset.targets[i] = New_Value\n",
        "      New_Values.append(New_Value)\n",
        "\n",
        "\n",
        "  return Changed_dataset\n"
      ],
      "metadata": {
        "id": "WPCYrE8yBn7m"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.functional as F\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "class fire(nn.Module):\n",
        "    def __init__(self, inplanes, squeeze_planes, expand_planes):\n",
        "        super(fire, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1, stride=1)\n",
        "        self.bn1 = nn.BatchNorm2d(squeeze_planes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(squeeze_planes, expand_planes, kernel_size=1, stride=1)\n",
        "        self.bn2 = nn.BatchNorm2d(expand_planes)\n",
        "        self.conv3 = nn.Conv2d(squeeze_planes, expand_planes, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(expand_planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        # using MSR initilization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2./n))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        out1 = self.conv2(x)\n",
        "        out1 = self.bn2(out1)\n",
        "        out2 = self.conv3(x)\n",
        "        out2 = self.bn3(out2)\n",
        "        out = torch.cat([out1, out2], 1)\n",
        "        out = self.relu2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 96, kernel_size=3, stride=1, padding=1) # 32\n",
        "        self.bn1 = nn.BatchNorm2d(96)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2) # 16\n",
        "        self.fire2 = fire(96, 16, 64)\n",
        "        self.fire3 = fire(128, 16, 64)\n",
        "        self.fire4 = fire(128, 32, 128)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2) # 8\n",
        "        self.fire5 = fire(256, 32, 128)\n",
        "        self.fire6 = fire(256, 48, 192)\n",
        "        self.fire7 = fire(384, 48, 192)\n",
        "        self.fire8 = fire(384, 64, 256)\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2) # 4\n",
        "        self.fire9 = fire(512, 64, 256)\n",
        "        self.conv2 = nn.Conv2d(512, 10, kernel_size=1, stride=1)\n",
        "        self.avg_pool = nn.AvgPool2d(kernel_size=4, stride=4)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.fire2(x)\n",
        "        x = self.fire3(x)\n",
        "        x = self.fire4(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.fire5(x)\n",
        "        x = self.fire6(x)\n",
        "        x = self.fire7(x)\n",
        "        x = self.fire8(x)\n",
        "        x = self.maxpool3(x)\n",
        "        x = self.fire9(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = self.softmax(x)\n",
        "        return x,x\n",
        "\n",
        "def fire_layer(inp, s, e):\n",
        "    f = fire(inp, s, e)\n",
        "    return f\n",
        "\n",
        "def squeezenet(pretrained=False):\n",
        "    net = SqueezeNet()\n",
        "    # inp = Variable(torch.randn(64,3,32,32))\n",
        "    # out = net.forward(inp)\n",
        "    # print(out.size())\n",
        "    return net\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     squeezenet()"
      ],
      "metadata": {
        "id": "daSOnO9CADeg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for epoch in range(num_epochs):\n",
        "def trainCNN(P_DML_ss = 0,P_DML = 1, P_SS =1, ss = False):  \n",
        "  net.train()\n",
        "  net2.train()\n",
        "\n",
        "  # total = 0\n",
        "  # correct = 0\n",
        "  # correct2 = 0\n",
        "\n",
        "# remove enumerate and _? \n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    \n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    guesses, _ = net.forward(images)\n",
        "    guesses = guesses.view(batch_size,10)\n",
        "    loss = loss_fn(guesses,labels)\n",
        "\n",
        "    guesses2, _ = net2.forward(images)\n",
        "    guesses2 = guesses2.view(batch_size,10)    \n",
        "    loss2 = loss_fn(guesses2,labels)\n",
        "    \n",
        "    #DML_Loss\n",
        "    # Aleternate depending on epoch?\n",
        "    loss_DML = P_DML*loss_fn(guesses,guesses2.softmax(dim=1).detach())\n",
        "    loss += loss_DML\n",
        "\n",
        "    loss_DML2 = P_DML*loss_fn(guesses2,guesses.softmax(dim=1).detach())\n",
        "    loss2 += loss_DML2\n",
        "\n",
        "    if ss == True: # Rotation Predictions\n",
        "      bx = images\n",
        "      \n",
        "      curr_batch_size = bx.size(0)\n",
        "      by_prime = torch.cat((torch.zeros(bx.size(0)), torch.ones(bx.size(0)),\n",
        "                                  2*torch.ones(bx.size(0)), 3*torch.ones(bx.size(0))), 0).long()\n",
        "      \n",
        "      bx = images\n",
        "      bx = torch.cat((bx, torch.rot90(bx,1,[2,3]),torch.rot90(bx, 2,[2,3]), torch.rot90(bx, 3,[2,3])), 0)\n",
        "      \n",
        "      bx, by_prime = bx.to(device), by_prime.to(device)\n",
        "\n",
        "      _, pen = net(bx)\n",
        "      _, pen2 = net2(bx)\n",
        "\n",
        "      ss_guesses = net.rot_pred(pen)\n",
        "      ss_guesses2 = net2.rot_pred(pen2)\n",
        "\n",
        "      ss_loss = P_SS*loss_fn(ss_guesses, by_prime)\n",
        "      ss_loss2 = P_SS*loss_fn(ss_guesses2, by_prime)\n",
        "\n",
        "      loss_DML_ss = P_DML_ss*loss_fn(ss_guesses,ss_guesses2.softmax(dim=1).detach())\n",
        "      loss += loss_DML_ss\n",
        "\n",
        "      loss_DML2_ss = P_DML_ss*loss_fn(ss_guesses2,ss_guesses.softmax(dim=1).detach())\n",
        "      loss2 += loss_DML2_ss\n",
        "\n",
        "\n",
        "      loss += ss_loss\n",
        "      loss2 += ss_loss2\n",
        "\n",
        "\n",
        "\n",
        "# Usually when you get a gradient in a step and then recalculate (for next step) it accumulates the gradient\n",
        "# This is actually useful for batch gradient descent when you can't do everything at once, but here we aren't memory constrained and want them to start at 0\n",
        "    optimizer.zero_grad() \n",
        "    \n",
        "    loss.backward()\n",
        "    loss2.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# loss.item just takes the loss written as a tensor and returns as  a float\n",
        "\n",
        "  scheduler.step(loss)\n",
        "#     Decay learning rate\n",
        "#    if (epoch+1) % 20 == 0:\n",
        "#        curr_lr /= 3\n",
        "#        update_lr(optimizer, curr_lr)\n",
        "\n",
        "\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "V3We_m6DADhG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testRes(test_loader = test_loader):\n",
        "  net.eval()\n",
        "  net2.eval()\n",
        "  with torch.no_grad():\n",
        "      correct = 0\n",
        "      total = 0\n",
        "\n",
        "      correct2 = 0\n",
        "      total2 = 0      \n",
        "\n",
        "      for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        guesses,_ = net.forward(images)\n",
        "        guesses = guesses.view(batch_size,10)\n",
        "        _, predicted = torch.max(guesses.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        test_accuracy = correct / total\n",
        "\n",
        "        guesses2,_ = net2(images)\n",
        "        _, predicted2 = torch.max(guesses2.data, 1)\n",
        "        total2 += labels.size(0)\n",
        "        correct2 += (predicted2 == labels).sum().item()\n",
        "  return test_accuracy"
      ],
      "metadata": {
        "id": "cWU1U4WtADjM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "device = torch.device('cuda')\n",
        "num_epochs = 10\n",
        "learning_rate = 0.1\n",
        "loss_fn = nn.CrossEntropyLoss() # Also an nn.module\n",
        "\n",
        "net_safe = squeezenet()\n",
        "net2_safe = squeezenet()\n",
        "\n",
        "net_safe.rot_pred  = nn.Linear(256, 4)\n",
        "net2_safe.rot_pred = nn.Linear(256, 4)\n",
        "\n",
        "net_safe.to(device)\n",
        "net2_safe.to(device)\n",
        "            \n",
        "            \n",
        "optimizer = torch.optim.SGD(list(net.parameters()) +list(net2.parameters()), lr= learning_rate) # An optimiser object\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 5)\n",
        "        "
      ],
      "metadata": {
        "id": "40FYnzLFA_KP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix of what I want to train:\n",
        "Corruption = [1,0.5,0.2]\n",
        "DML = [0,1]\n",
        "\n",
        "Type = ['Unchanged','DML']"
      ],
      "metadata": {
        "id": "uj9mNkJeCJwU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "All_accs = []\n",
        "for k in range(3):\n",
        "    for j in range(2):\n",
        "        for i in range(3):\n",
        "            \n",
        "            p = Corruption[i]\n",
        "            Changed_dataset = Changer(p)\n",
        "            Changed_loader = torch.utils.data.DataLoader(dataset=Changed_dataset,\n",
        "                                                batch_size= batch_size, \n",
        "                                                shuffle=True, num_workers = num_workers)\n",
        "    \n",
        "            net = copy.deepcopy(net_safe)\n",
        "            net2 = copy.deepcopy(net2_safe)\n",
        "            \n",
        "            net.to(device)\n",
        "            net2.to(device)\n",
        "            \n",
        "            \n",
        "            \n",
        "            optimizer = torch.optim.SGD(list(net.parameters()) +list(net2.parameters()), lr= learning_rate) # An optimiser object\n",
        "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 5)\n",
        "              \n",
        "            loss = np.array([])\n",
        "            acc = np.array([])\n",
        "              \n",
        "            for epoch in range(num_epochs):\n",
        "                  \n",
        "                begin_epoch = time.time()\\\n",
        "                  \n",
        "                new_train_loss = trainCNN(P_DML_ss= 0, P_DML = DML[j], P_SS = 0,ss = False)\n",
        "                \n",
        "                \n",
        "                loss = np.append(loss,new_train_loss.cpu().detach().numpy())\n",
        "                new_acc = testRes()\n",
        "                acc = np.append(acc, new_acc)\n",
        "              \n",
        "                print('Epoch', epoch, '| Time Spent:', round(time.time() - begin_epoch, 2), 'Train_Loss:',new_train_loss, 'Test_Accuracy1:', new_acc)\n",
        "            print(Corruption[i])\n",
        "            print(Type[j])\n",
        "            print(acc)\n",
        "            All_accs.append((acc,Type[j]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHVmwgFsBBJQ",
        "outputId": "dbab765a-52bc-4815-f652-d4b3a2448664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Time Spent: 31.39 Train_Loss: tensor(1.1737, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.493\n",
            "Epoch 1 | Time Spent: 31.22 Train_Loss: tensor(1.0402, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5818\n",
            "Epoch 2 | Time Spent: 31.34 Train_Loss: tensor(0.9245, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5843\n",
            "Epoch 3 | Time Spent: 31.31 Train_Loss: tensor(0.8743, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.628\n",
            "Epoch 4 | Time Spent: 31.29 Train_Loss: tensor(0.8751, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6449\n",
            "Epoch 5 | Time Spent: 31.22 Train_Loss: tensor(0.7830, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6979\n",
            "Epoch 6 | Time Spent: 31.3 Train_Loss: tensor(0.6983, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6465\n",
            "Epoch 7 | Time Spent: 31.43 Train_Loss: tensor(0.8219, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6704\n",
            "Epoch 8 | Time Spent: 31.38 Train_Loss: tensor(0.6142, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6868\n",
            "Epoch 9 | Time Spent: 31.69 Train_Loss: tensor(0.4190, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6343\n",
            "1\n",
            "Unchanged\n",
            "[0.493  0.5818 0.5843 0.628  0.6449 0.6979 0.6465 0.6704 0.6868 0.6343]\n",
            "Epoch 0 | Time Spent: 31.23 Train_Loss: tensor(1.3285, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.4724\n",
            "Epoch 1 | Time Spent: 31.48 Train_Loss: tensor(1.1100, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5554\n",
            "Epoch 2 | Time Spent: 31.24 Train_Loss: tensor(0.9481, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6033\n",
            "Epoch 3 | Time Spent: 31.31 Train_Loss: tensor(0.8922, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6465\n",
            "Epoch 4 | Time Spent: 31.17 Train_Loss: tensor(0.7722, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6684\n",
            "Epoch 5 | Time Spent: 31.3 Train_Loss: tensor(0.8193, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.7018\n",
            "Epoch 6 | Time Spent: 31.19 Train_Loss: tensor(0.8281, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6689\n",
            "Epoch 7 | Time Spent: 31.41 Train_Loss: tensor(0.6150, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6631\n",
            "Epoch 8 | Time Spent: 31.7 Train_Loss: tensor(0.5987, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6737\n",
            "Epoch 9 | Time Spent: 31.39 Train_Loss: tensor(0.4636, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6822\n",
            "0.5\n",
            "Unchanged\n",
            "[0.4724 0.5554 0.6033 0.6465 0.6684 0.7018 0.6689 0.6631 0.6737 0.6822]\n",
            "Epoch 0 | Time Spent: 31.63 Train_Loss: tensor(1.3720, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.4982\n",
            "Epoch 1 | Time Spent: 31.42 Train_Loss: tensor(1.0703, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5759\n",
            "Epoch 2 | Time Spent: 31.61 Train_Loss: tensor(0.9934, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5688\n",
            "Epoch 3 | Time Spent: 31.35 Train_Loss: tensor(0.8512, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6415\n",
            "Epoch 4 | Time Spent: 31.37 Train_Loss: tensor(0.5991, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6781\n",
            "Epoch 5 | Time Spent: 31.38 Train_Loss: tensor(0.6424, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6941\n",
            "Epoch 6 | Time Spent: 31.45 Train_Loss: tensor(0.6445, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6865\n",
            "Epoch 7 | Time Spent: 31.38 Train_Loss: tensor(0.6703, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.7001\n",
            "Epoch 8 | Time Spent: 31.49 Train_Loss: tensor(0.5612, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6764\n",
            "Epoch 9 | Time Spent: 31.85 Train_Loss: tensor(0.3952, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.7313\n",
            "0.2\n",
            "Unchanged\n",
            "[0.4982 0.5759 0.5688 0.6415 0.6781 0.6941 0.6865 0.7001 0.6764 0.7313]\n",
            "Epoch 0 | Time Spent: 31.23 Train_Loss: tensor(2.9768, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.4954\n",
            "Epoch 1 | Time Spent: 31.55 Train_Loss: tensor(2.7155, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5265\n",
            "Epoch 2 | Time Spent: 31.22 Train_Loss: tensor(2.6748, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5645\n",
            "Epoch 3 | Time Spent: 31.42 Train_Loss: tensor(2.2482, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6487\n",
            "Epoch 4 | Time Spent: 31.3 Train_Loss: tensor(2.0472, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6498\n",
            "Epoch 5 | Time Spent: 31.47 Train_Loss: tensor(1.8303, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6795\n",
            "Epoch 6 | Time Spent: 31.38 Train_Loss: tensor(1.7257, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6717\n",
            "Epoch 7 | Time Spent: 31.39 Train_Loss: tensor(1.8143, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6771\n",
            "Epoch 8 | Time Spent: 31.37 Train_Loss: tensor(1.6543, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6871\n",
            "Epoch 9 | Time Spent: 31.36 Train_Loss: tensor(1.6102, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.7031\n",
            "1\n",
            "DML\n",
            "[0.4954 0.5265 0.5645 0.6487 0.6498 0.6795 0.6717 0.6771 0.6871 0.7031]\n",
            "Epoch 0 | Time Spent: 31.97 Train_Loss: tensor(3.0869, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.4356\n",
            "Epoch 1 | Time Spent: 31.32 Train_Loss: tensor(2.8796, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.4975\n",
            "Epoch 2 | Time Spent: 31.51 Train_Loss: tensor(2.5091, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5597\n",
            "Epoch 3 | Time Spent: 31.31 Train_Loss: tensor(1.9572, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6018\n",
            "Epoch 4 | Time Spent: 31.36 Train_Loss: tensor(2.0447, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6575\n",
            "Epoch 5 | Time Spent: 31.24 Train_Loss: tensor(2.0176, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.6699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FHztbnVfBEEm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}