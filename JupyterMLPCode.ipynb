{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLLiJuYh9QgFBmGa1Ry2xC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mhobo/SimNets/blob/main/JupyterMLPCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rnkys9OroCIV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "import random as rnd\n",
        "from scipy.stats import bernoulli\n",
        "import time\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_workers = 8"
      ],
      "metadata": {
        "id": "Of4DY5QooDcI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FashionMNIST dataset\n",
        "# Change to Cifar 10\n",
        "# Dataset objects and Dataloader objects are objects Python defines \n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='data/',\n",
        "                                             train=True, \n",
        "                                             transform=transforms.ToTensor(),  #replace with = transform after\n",
        "                                             download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='data/',\n",
        "                                            train=False,#need comma here to get transforms back\n",
        "                                            transform=transforms.ToTensor())\n",
        "\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                            batch_size= batch_size, \n",
        "                                            shuffle=True, num_workers = num_workers)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                            batch_size= batch_size, \n",
        "                                            shuffle=False, num_workers = num_workers)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnzRfcoGoKpE",
        "outputId": "f77b0960-72f7-4909-ae71-a60aafb985a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:07<00:00, 21500467.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Changer(p = 0.5):\n",
        "\n",
        "#  Changed = range(0,6000,Ratio)\n",
        "  Changed_dataset = copy.deepcopy(train_dataset)\n",
        "# set new labels that are wrong\n",
        "  New_Values = []\n",
        "  \n",
        "  for i in range(len(Changed_dataset.targets)): #[Changed]\n",
        "    if bernoulli.rvs(p) == 1:\n",
        "      continue\n",
        "    else:\n",
        "\n",
        "      Label_Options = list(range(0,10))\n",
        "      Label_Options.remove(Changed_dataset.targets[i]) #Changed[i]\n",
        "      New_Value = rnd.choice(Label_Options)\n",
        "      Changed_dataset.targets[i] = New_Value\n",
        "      New_Values.append(New_Value)\n",
        "\n",
        "\n",
        "  return Changed_dataset\n"
      ],
      "metadata": {
        "id": "Hn7th8aAoKto"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for epoch in range(num_epochs):\n",
        "def train(P_DML_ss = 0,P_DML = 1, P_SS =1, ss = False):  \n",
        "  net.train()\n",
        "  net2.train()\n",
        "\n",
        "  # total = 0\n",
        "  # correct = 0\n",
        "  # correct2 = 0\n",
        "\n",
        "# Make Changer return the loader\n",
        "#   Changed_dataset = Changer(p)\n",
        "#   Changed_loader = torch.utils.data.DataLoader(dataset=Changed_dataset,\n",
        "#                                             batch_size= batch_size, \n",
        "#                                             shuffle=True, num_workers = num_workers)\n",
        "\n",
        "# remove enumerate and _? \n",
        "  for i, (images, labels) in enumerate(Changed_loader):\n",
        "    \n",
        "    images_flat = torch.flatten(images.to(device),start_dim = 1) # Will want to flatten this in  my case\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    guesses, _ = net(images_flat)\n",
        "    loss = loss_fn(guesses,labels)\n",
        "\n",
        "    guesses2, _ = net2(images_flat)\n",
        "    loss2 = loss_fn(guesses2,labels)\n",
        "    \n",
        "    #DML_Loss\n",
        "    # Aleternate depending on epoch?\n",
        "    loss_DML = P_DML*loss_fn(guesses,guesses2.softmax(dim=1).detach())\n",
        "    loss += loss_DML\n",
        "\n",
        "    loss_DML2 = P_DML*loss_fn(guesses2,guesses.softmax(dim=1).detach())\n",
        "    loss2 += loss_DML2\n",
        "\n",
        "    if ss == True: # Rotation Predictions\n",
        "      bx = images_flat\n",
        "      \n",
        "      curr_batch_size = bx.size(0)\n",
        "      by_prime = torch.cat((torch.zeros(bx.size(0)), torch.ones(bx.size(0)),\n",
        "                                  2*torch.ones(bx.size(0)), 3*torch.ones(bx.size(0))), 0).long()\n",
        "      \n",
        "      bx = images\n",
        "      bx = torch.cat((bx, torch.rot90(bx,1,[2,3]),torch.rot90(bx, 2,[2,3]), torch.rot90(bx, 3,[2,3])), 0)\n",
        "      bx = torch.flatten(bx,start_dim=1)\n",
        "      \n",
        "      bx, by_prime = bx.to(device), by_prime.to(device)\n",
        "\n",
        "      _, pen = net(bx)\n",
        "      _, pen2 = net2(bx)\n",
        "\n",
        "      ss_guesses = net.rot_pred(pen)\n",
        "      ss_guesses2 = net2.rot_pred(pen2)\n",
        "\n",
        "      ss_loss = P_SS*loss_fn(ss_guesses, by_prime)\n",
        "      ss_loss2 = P_SS*loss_fn(ss_guesses2, by_prime)\n",
        "\n",
        "      loss_DML_ss = P_DML_ss*loss_fn(ss_guesses,ss_guesses2.softmax(dim=1).detach())\n",
        "      loss += loss_DML_ss\n",
        "\n",
        "      loss_DML2_ss = P_DML_ss*loss_fn(ss_guesses2,ss_guesses.softmax(dim=1).detach())\n",
        "      loss2 += loss_DML2_ss\n",
        "\n",
        "\n",
        "      loss += ss_loss\n",
        "      loss2 += ss_loss2\n",
        "\n",
        "\n",
        "\n",
        "# Usually when you get a gradient in a step and then recalculate (for next step) it accumulates the gradient\n",
        "# This is actually useful for batch gradient descent when you can't do everything at once, but here we aren't memory constrained and want them to start at 0\n",
        "    optimizer.zero_grad() \n",
        "    \n",
        "    loss.backward()\n",
        "    loss2.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# loss.item just takes the loss written as a tensor and returns as  a float\n",
        "\n",
        "  scheduler.step(loss)\n",
        "#     Decay learning rate\n",
        "#    if (epoch+1) % 20 == 0:\n",
        "#        curr_lr /= 3\n",
        "#        update_lr(optimizer, curr_lr)\n",
        "\n",
        "\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "5r6sX7SVoKwr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(test_loader = test_loader):\n",
        "  net.eval()\n",
        "  net2.eval()\n",
        "  with torch.no_grad():\n",
        "      correct = 0\n",
        "      total = 0\n",
        "\n",
        "      correct2 = 0\n",
        "      total2 = 0      \n",
        "\n",
        "      for images, labels in test_loader:\n",
        "        images = torch.flatten(images.to(device),start_dim = 1)\n",
        "        labels = labels.to(device)\n",
        "        guesses,_ = net(images)\n",
        "        _, predicted = torch.max(guesses.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        test_accuracy = correct / total\n",
        "\n",
        "        guesses2,_ = net2(images)\n",
        "        _, predicted2 = torch.max(guesses2.data, 1)\n",
        "        total2 += labels.size(0)\n",
        "        correct2 += (predicted2 == labels).sum().item()\n",
        "  return test_accuracy"
      ],
      "metadata": {
        "id": "V-dgiDWnoSYH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')\n",
        "num_epochs = 40\n",
        "learning_rate = 0.1\n",
        "loss_fn = nn.CrossEntropyLoss() # Also an nn.module\n"
      ],
      "metadata": {
        "id": "ISKHZFw1oSc1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the layers we want in between, need somewhere to check dimension of dataset?\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self,layer_widths): \n",
        "# Alternatively \n",
        "    super().__init__()\n",
        "    self.m = len(layer_widths) #self stores it as characteristic of this NN\n",
        "    self.layers = [] # Empty list to store layers\n",
        "    self.non_lin = nn.ReLU() # nn.ReLU isn't the function, but the function that constructs a relu (by saying =)\n",
        "    for i in range(self.m-1): #does this make m-1 iterations?\n",
        "      self.layers.append(nn.Linear(layer_widths[i], layer_widths[i+1])) # Need matrix maths to work, so have (a,b) thenn (b,c)\n",
        "      self.add_module(\"layer\"+str(i), self.layers[-1])\n",
        "    self.do = nn.Dropout(0.2) \n",
        "\n",
        "  def forward(self, x):\n",
        "    z = x\n",
        "    for i in range(self.m - 1):\n",
        "        y = z\n",
        "        z = self.do(z)\n",
        "        z = self.non_lin(self.layers[i](z))\n",
        "        \n",
        "    return z,y\n",
        "# Probably better off hardcoding, sanity check and because the layers are designed with the dataset in mind"
      ],
      "metadata": {
        "id": "ecG8fslooX6U"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining loss and optimiser\n",
        "torch.manual_seed(0)\n",
        "# m = 3\n",
        "# Inside = [50]*m\n",
        "# Inside.insert(0,(28*28))\n",
        "# Inside.append(10)\n",
        "Inside = [(3*32*32),1536,768,384,192,96,10] # [128,10] good, [128,64, 32,10]] also gets to 87% in 15 epochs, SSL and DML drop 10% accuracy for uncorrupted data for first class at that size\n",
        "# For 0.5 corruption SS does a lot better, 55% for vanilla and DML, 75% for SS\n",
        "# 32,10 suggested here: https://www.kaggle.com/code/pavansanagapati/a-simple-cnn-model-beginner-guide\n",
        "net_safe = MLP(Inside)\n",
        "net2_safe = MLP(Inside)\n",
        "\n",
        "net_safe.rot_pred  = nn.Linear(96, 4)\n",
        "net2_safe.rot_pred = nn.Linear(96, 4)\n",
        "\n",
        "net_safe.to(device)\n",
        "net2_safe.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss() # Also an nn.module\n",
        "# 5 layer Cifar10 [(32*32*3),1024,512,128,10], 1536,768,384,192,96"
      ],
      "metadata": {
        "id": "zhCFe6bNoX90"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix of what I want to train:\n",
        "\n",
        "Corruption = [1,0.5,0.2]\n",
        "# removed first line of 0s from each so we can see performace\n",
        "DML = [0,1,0,1,0]\n",
        "SSL = [0,0,1,1,1]\n",
        "DML_SSL = [0,0,0,0,1]\n",
        "Type = ['Unchanged','DML','SSL','Out','In']"
      ],
      "metadata": {
        "id": "KLwZ_6KmogLt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "All_accs = []\n",
        "for k in range(3):\n",
        "    for j in range(5):\n",
        "        #for i in range(3):\n",
        "            \n",
        "            p = 0.2 #Corruption[i]\n",
        "            Changed_dataset = Changer(p)\n",
        "            Changed_loader = torch.utils.data.DataLoader(dataset=Changed_dataset,\n",
        "                                                batch_size= batch_size, \n",
        "                                                shuffle=True, num_workers = num_workers)\n",
        "    \n",
        "            net = copy.deepcopy(net_safe)\n",
        "            net2 = copy.deepcopy(net2_safe)\n",
        "            \n",
        "            net.to(device)\n",
        "            net2.to(device)\n",
        "            \n",
        "            \n",
        "            \n",
        "            optimizer = torch.optim.SGD(list(net.parameters()) +list(net2.parameters()), lr= learning_rate, momentum = 0.5, weight_decay = 0.0005) # An optimiser object\n",
        "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 5)\n",
        "              \n",
        "            loss = np.array([])\n",
        "            acc = np.array([])\n",
        "              \n",
        "            for epoch in range(num_epochs):\n",
        "                  \n",
        "                begin_epoch = time.time()\\\n",
        "                  \n",
        "                new_train_loss = train(P_DML_ss= DML_SSL[j], P_DML = DML[j], P_SS = SSL[j],ss = True)\n",
        "                \n",
        "                \n",
        "                loss = np.append(loss,new_train_loss.cpu().detach().numpy())\n",
        "                new_acc = test()\n",
        "                acc = np.append(acc, new_acc)\n",
        "              \n",
        "                print('Epoch', epoch, '| Time Spent:', round(time.time() - begin_epoch, 2), 'Train_Loss:',new_train_loss, 'Test_Accuracy1:', new_acc)\n",
        "            #print(Corruption[i])\n",
        "            print(Type[j])\n",
        "            print(acc)\n",
        "            All_accs.append((acc,Type[j]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmnu2uTboiuE",
        "outputId": "86ffecd1-490f-4d98-fd3c-f24bab06705a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Time Spent: 16.04 Train_Loss: tensor(4.6052, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "50vFTIoXoqXx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}