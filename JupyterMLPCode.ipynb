{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnz8sZmAdIojOcvWgoFcHx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mhobo/SimNets/blob/main/JupyterMLPCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rnkys9OroCIV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "import random as rnd\n",
        "from scipy.stats import bernoulli\n",
        "import time\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_workers = 8"
      ],
      "metadata": {
        "id": "Of4DY5QooDcI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FashionMNIST dataset\n",
        "# Change to Cifar 10\n",
        "# Dataset objects and Dataloader objects are objects Python defines \n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='data/',\n",
        "                                             train=True, \n",
        "                                             transform=transforms.ToTensor(),  #replace with = transform after\n",
        "                                             download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='data/',\n",
        "                                            train=False,#need comma here to get transforms back\n",
        "                                            transform=transforms.ToTensor())\n",
        "\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                            batch_size= batch_size, \n",
        "                                            shuffle=True, num_workers = num_workers)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                            batch_size= batch_size, \n",
        "                                            shuffle=False, num_workers = num_workers)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnzRfcoGoKpE",
        "outputId": "f77b0960-72f7-4909-ae71-a60aafb985a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:07<00:00, 21500467.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Changer(p = 0.5):\n",
        "\n",
        "#  Changed = range(0,6000,Ratio)\n",
        "  Changed_dataset = copy.deepcopy(train_dataset)\n",
        "# set new labels that are wrong\n",
        "  New_Values = []\n",
        "  \n",
        "  for i in range(len(Changed_dataset.targets)): #[Changed]\n",
        "    if bernoulli.rvs(p) == 1:\n",
        "      continue\n",
        "    else:\n",
        "\n",
        "      Label_Options = list(range(0,10))\n",
        "      Label_Options.remove(Changed_dataset.targets[i]) #Changed[i]\n",
        "      New_Value = rnd.choice(Label_Options)\n",
        "      Changed_dataset.targets[i] = New_Value\n",
        "      New_Values.append(New_Value)\n",
        "\n",
        "\n",
        "  return Changed_dataset\n"
      ],
      "metadata": {
        "id": "Hn7th8aAoKto"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for epoch in range(num_epochs):\n",
        "def train(P_DML_ss = 0,P_DML = 1, P_SS =1, ss = False):  \n",
        "  net.train()\n",
        "  net2.train()\n",
        "\n",
        "  # total = 0\n",
        "  # correct = 0\n",
        "  # correct2 = 0\n",
        "\n",
        "# Make Changer return the loader\n",
        "#   Changed_dataset = Changer(p)\n",
        "#   Changed_loader = torch.utils.data.DataLoader(dataset=Changed_dataset,\n",
        "#                                             batch_size= batch_size, \n",
        "#                                             shuffle=True, num_workers = num_workers)\n",
        "\n",
        "# remove enumerate and _? \n",
        "  for i, (images, labels) in enumerate(Changed_loader):\n",
        "    \n",
        "    images_flat = torch.flatten(images.to(device),start_dim = 1) # Will want to flatten this in  my case\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    guesses, _ = net(images_flat)\n",
        "    loss = loss_fn(guesses,labels)\n",
        "\n",
        "    guesses2, _ = net2(images_flat)\n",
        "    loss2 = loss_fn(guesses2,labels)\n",
        "    \n",
        "    #DML_Loss\n",
        "    # Aleternate depending on epoch?\n",
        "    loss_DML = P_DML*loss_fn(guesses,guesses2.softmax(dim=1).detach())\n",
        "    loss += loss_DML\n",
        "\n",
        "    loss_DML2 = P_DML*loss_fn(guesses2,guesses.softmax(dim=1).detach())\n",
        "    loss2 += loss_DML2\n",
        "\n",
        "    if ss == True: # Rotation Predictions\n",
        "      bx = images_flat\n",
        "      \n",
        "      curr_batch_size = bx.size(0)\n",
        "      by_prime = torch.cat((torch.zeros(bx.size(0)), torch.ones(bx.size(0)),\n",
        "                                  2*torch.ones(bx.size(0)), 3*torch.ones(bx.size(0))), 0).long()\n",
        "      \n",
        "      bx = images\n",
        "      bx = torch.cat((bx, torch.rot90(bx,1,[2,3]),torch.rot90(bx, 2,[2,3]), torch.rot90(bx, 3,[2,3])), 0)\n",
        "      bx = torch.flatten(bx,start_dim=1)\n",
        "      \n",
        "      bx, by_prime = bx.to(device), by_prime.to(device)\n",
        "\n",
        "      _, pen = net(bx)\n",
        "      _, pen2 = net2(bx)\n",
        "\n",
        "      ss_guesses = net.rot_pred(pen)\n",
        "      ss_guesses2 = net2.rot_pred(pen2)\n",
        "\n",
        "      ss_loss = P_SS*loss_fn(ss_guesses, by_prime)\n",
        "      ss_loss2 = P_SS*loss_fn(ss_guesses2, by_prime)\n",
        "\n",
        "      loss_DML_ss = P_DML_ss*loss_fn(ss_guesses,ss_guesses2.softmax(dim=1).detach())\n",
        "      loss += loss_DML_ss\n",
        "\n",
        "      loss_DML2_ss = P_DML_ss*loss_fn(ss_guesses2,ss_guesses.softmax(dim=1).detach())\n",
        "      loss2 += loss_DML2_ss\n",
        "\n",
        "\n",
        "      loss += ss_loss\n",
        "      loss2 += ss_loss2\n",
        "\n",
        "\n",
        "\n",
        "# Usually when you get a gradient in a step and then recalculate (for next step) it accumulates the gradient\n",
        "# This is actually useful for batch gradient descent when you can't do everything at once, but here we aren't memory constrained and want them to start at 0\n",
        "    optimizer.zero_grad() \n",
        "    \n",
        "    loss.backward()\n",
        "    loss2.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# loss.item just takes the loss written as a tensor and returns as  a float\n",
        "\n",
        "  scheduler.step(loss)\n",
        "#     Decay learning rate\n",
        "#    if (epoch+1) % 20 == 0:\n",
        "#        curr_lr /= 3\n",
        "#        update_lr(optimizer, curr_lr)\n",
        "\n",
        "\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "5r6sX7SVoKwr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(test_loader = test_loader):\n",
        "  net.eval()\n",
        "  net2.eval()\n",
        "  with torch.no_grad():\n",
        "      correct = 0\n",
        "      total = 0\n",
        "\n",
        "      correct2 = 0\n",
        "      total2 = 0      \n",
        "\n",
        "      for images, labels in test_loader:\n",
        "        images = torch.flatten(images.to(device),start_dim = 1)\n",
        "        labels = labels.to(device)\n",
        "        guesses,_ = net(images)\n",
        "        _, predicted = torch.max(guesses.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        test_accuracy = correct / total\n",
        "\n",
        "        guesses2,_ = net2(images)\n",
        "        _, predicted2 = torch.max(guesses2.data, 1)\n",
        "        total2 += labels.size(0)\n",
        "        correct2 += (predicted2 == labels).sum().item()\n",
        "  return test_accuracy"
      ],
      "metadata": {
        "id": "V-dgiDWnoSYH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')\n",
        "num_epochs = 50\n",
        "learning_rate = 0.1\n",
        "loss_fn = nn.CrossEntropyLoss() # Also an nn.module\n"
      ],
      "metadata": {
        "id": "ISKHZFw1oSc1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the layers we want in between, need somewhere to check dimension of dataset?\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self,layer_widths): \n",
        "# Alternatively \n",
        "    super().__init__()\n",
        "    self.m = len(layer_widths) #self stores it as characteristic of this NN\n",
        "    self.layers = [] # Empty list to store layers\n",
        "    self.non_lin = nn.ReLU() # nn.ReLU isn't the function, but the function that constructs a relu (by saying =)\n",
        "    for i in range(self.m-1): #does this make m-1 iterations?\n",
        "      self.layers.append(nn.Linear(layer_widths[i], layer_widths[i+1])) # Need matrix maths to work, so have (a,b) thenn (b,c)\n",
        "      self.add_module(\"layer\"+str(i), self.layers[-1])\n",
        "    self.do = nn.Dropout(0.2) \n",
        "\n",
        "  def forward(self, x):\n",
        "    z = x\n",
        "    for i in range(self.m - 1):\n",
        "        y = z\n",
        "        z = self.do(z)\n",
        "        z = self.non_lin(self.layers[i](z))\n",
        "        \n",
        "    return z,y\n",
        "# Probably better off hardcoding, sanity check and because the layers are designed with the dataset in mind"
      ],
      "metadata": {
        "id": "ecG8fslooX6U"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining loss and optimiser\n",
        "torch.manual_seed(0)\n",
        "m = 3\n",
        "Inside = [50]*m\n",
        "Inside.insert(0,(28*28))\n",
        "Inside.append(10)\n",
        "Inside = [(3*32*32),1536,768,384,192,96,10] # [128,10] good, [128,64, 32,10]] also gets to 87% in 15 epochs, SSL and DML drop 10% accuracy for uncorrupted data for first class at that size\n",
        "# For 0.5 corruption SS does a lot better, 55% for vanilla and DML, 75% for SS\n",
        "# 32,10 suggested here: https://www.kaggle.com/code/pavansanagapati/a-simple-cnn-model-beginner-guide\n",
        "net_safe = MLP(Inside)\n",
        "net2_safe = MLP(Inside)\n",
        "\n",
        "net_safe.rot_pred  = nn.Linear(96, 4)\n",
        "net2_safe.rot_pred = nn.Linear(96, 4)\n",
        "\n",
        "net_safe.to(device)\n",
        "net2_safe.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss() # Also an nn.module\n",
        "# 5 layer Cifar10 [(32*32*3),1024,512,128,10], 1536,768,384,192,96"
      ],
      "metadata": {
        "id": "zhCFe6bNoX90"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix of what I want to train:\n",
        "\n",
        "Corruption = [1,0.5,0.2]\n",
        "DML = [0,1,0,1,0]\n",
        "SSL = [0,0,1,1,1]\n",
        "DML_SSL = [0,0,0,0,1]\n",
        "Type = ['Unchanged','DML','SSL','Out','In']"
      ],
      "metadata": {
        "id": "KLwZ_6KmogLt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "All_accs = []\n",
        "for k in range(3):\n",
        "    for j in range(5):\n",
        "        for i in range(3):\n",
        "            \n",
        "            p = Corruption[i]\n",
        "            Changed_dataset = Changer(p)\n",
        "            Changed_loader = torch.utils.data.DataLoader(dataset=Changed_dataset,\n",
        "                                                batch_size= batch_size, \n",
        "                                                shuffle=True, num_workers = num_workers)\n",
        "    \n",
        "            net = copy.deepcopy(net_safe)\n",
        "            net2 = copy.deepcopy(net2_safe)\n",
        "            \n",
        "            net.to(device)\n",
        "            net2.to(device)\n",
        "            \n",
        "            \n",
        "            \n",
        "            optimizer = torch.optim.SGD(list(net.parameters()) +list(net2.parameters()), lr= learning_rate) # An optimiser object\n",
        "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 5)\n",
        "              \n",
        "            loss = np.array([])\n",
        "            acc = np.array([])\n",
        "              \n",
        "            for epoch in range(num_epochs):\n",
        "                  \n",
        "                begin_epoch = time.time()\\\n",
        "                  \n",
        "                new_train_loss = train(P_DML_ss= DML_SSL[j], P_DML = DML[j], P_SS = SSL[j],ss = True)\n",
        "                \n",
        "                \n",
        "                loss = np.append(loss,new_train_loss.cpu().detach().numpy())\n",
        "                new_acc = test()\n",
        "                acc = np.append(acc, new_acc)\n",
        "              \n",
        "                print('Epoch', epoch, '| Time Spent:', round(time.time() - begin_epoch, 2), 'Train_Loss:',new_train_loss, 'Test_Accuracy1:', new_acc)\n",
        "            print(Corruption[i])\n",
        "            print(Type[j])\n",
        "            print(acc)\n",
        "            All_accs.append((acc,Type[j]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmnu2uTboiuE",
        "outputId": "1e6eed7d-070f-4a85-b234-0bc69ba46332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Time Spent: 15.56 Train_Loss: tensor(2.2998, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.1\n",
            "Epoch 1 | Time Spent: 15.9 Train_Loss: tensor(2.2635, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.1385\n",
            "Epoch 2 | Time Spent: 14.98 Train_Loss: tensor(2.1099, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.1824\n",
            "Epoch 3 | Time Spent: 14.65 Train_Loss: tensor(2.1565, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.2161\n",
            "Epoch 4 | Time Spent: 14.66 Train_Loss: tensor(2.0295, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.2674\n",
            "Epoch 5 | Time Spent: 14.37 Train_Loss: tensor(1.8436, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.3446\n",
            "Epoch 6 | Time Spent: 14.62 Train_Loss: tensor(1.9574, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.3543\n",
            "Epoch 7 | Time Spent: 15.89 Train_Loss: tensor(1.7675, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.368\n",
            "Epoch 8 | Time Spent: 15.13 Train_Loss: tensor(1.6102, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.3931\n",
            "Epoch 9 | Time Spent: 14.54 Train_Loss: tensor(1.8536, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.3967\n",
            "Epoch 10 | Time Spent: 15.1 Train_Loss: tensor(1.6887, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.4184\n",
            "Epoch 11 | Time Spent: 14.67 Train_Loss: tensor(1.7250, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.4413\n",
            "Epoch 12 | Time Spent: 14.6 Train_Loss: tensor(1.5570, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.4305\n",
            "Epoch 13 | Time Spent: 14.75 Train_Loss: tensor(1.5132, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.4631\n",
            "Epoch 14 | Time Spent: 14.57 Train_Loss: tensor(1.4469, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.4534\n",
            "Epoch 15 | Time Spent: 14.74 Train_Loss: tensor(1.5146, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.4494\n",
            "Epoch 16 | Time Spent: 14.95 Train_Loss: tensor(1.7764, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.4342\n",
            "Epoch 17 | Time Spent: 15.26 Train_Loss: tensor(1.4265, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.4663\n",
            "Epoch 18 | Time Spent: 14.59 Train_Loss: tensor(1.5287, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.4678\n",
            "Epoch 19 | Time Spent: 14.59 Train_Loss: tensor(1.5808, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.4692\n",
            "Epoch 20 | Time Spent: 14.61 Train_Loss: tensor(1.3488, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.4733\n",
            "Epoch 21 | Time Spent: 14.81 Train_Loss: tensor(1.3867, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.4843\n",
            "Epoch 22 | Time Spent: 14.7 Train_Loss: tensor(1.3520, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.4873\n",
            "Epoch 23 | Time Spent: 14.69 Train_Loss: tensor(1.4907, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.4908\n",
            "Epoch 24 | Time Spent: 14.59 Train_Loss: tensor(1.2851, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.4576\n",
            "Epoch 25 | Time Spent: 15.06 Train_Loss: tensor(1.4084, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5128\n",
            "Epoch 26 | Time Spent: 15.39 Train_Loss: tensor(1.2812, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5159\n",
            "Epoch 27 | Time Spent: 14.59 Train_Loss: tensor(1.3872, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5077\n",
            "Epoch 28 | Time Spent: 14.65 Train_Loss: tensor(1.1970, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.488\n",
            "Epoch 29 | Time Spent: 14.65 Train_Loss: tensor(1.2584, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5188\n",
            "Epoch 30 | Time Spent: 14.75 Train_Loss: tensor(1.4421, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5145\n",
            "Epoch 31 | Time Spent: 14.68 Train_Loss: tensor(1.2880, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5181\n",
            "Epoch 32 | Time Spent: 14.87 Train_Loss: tensor(1.3652, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.4691\n",
            "Epoch 33 | Time Spent: 14.65 Train_Loss: tensor(1.4277, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.4908\n",
            "Epoch 34 | Time Spent: 14.63 Train_Loss: tensor(1.2568, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5025\n",
            "Epoch 35 | Time Spent: 15.29 Train_Loss: tensor(1.1442, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5494\n",
            "Epoch 36 | Time Spent: 14.87 Train_Loss: tensor(1.3112, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5534\n",
            "Epoch 37 | Time Spent: 14.54 Train_Loss: tensor(0.9696, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.556\n",
            "Epoch 38 | Time Spent: 14.62 Train_Loss: tensor(0.9513, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5558\n",
            "Epoch 39 | Time Spent: 14.59 Train_Loss: tensor(0.8742, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5581\n",
            "Epoch 40 | Time Spent: 14.75 Train_Loss: tensor(1.1488, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5565\n",
            "Epoch 41 | Time Spent: 14.76 Train_Loss: tensor(1.1439, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5596\n",
            "Epoch 42 | Time Spent: 14.51 Train_Loss: tensor(1.1665, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5614\n",
            "Epoch 43 | Time Spent: 14.66 Train_Loss: tensor(1.1060, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5585\n",
            "Epoch 44 | Time Spent: 14.64 Train_Loss: tensor(1.2210, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5618\n",
            "Epoch 45 | Time Spent: 15.54 Train_Loss: tensor(1.0664, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5608\n",
            "Epoch 46 | Time Spent: 14.87 Train_Loss: tensor(0.8983, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5624\n",
            "Epoch 47 | Time Spent: 14.68 Train_Loss: tensor(1.0878, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.563\n",
            "Epoch 48 | Time Spent: 14.68 Train_Loss: tensor(1.0888, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5632\n",
            "Epoch 49 | Time Spent: 14.72 Train_Loss: tensor(1.0825, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.5647\n",
            "1\n",
            "Unchanged\n",
            "[0.1    0.1385 0.1824 0.2161 0.2674 0.3446 0.3543 0.368  0.3931 0.3967\n",
            " 0.4184 0.4413 0.4305 0.4631 0.4534 0.4494 0.4342 0.4663 0.4678 0.4692\n",
            " 0.4733 0.4843 0.4873 0.4908 0.4576 0.5128 0.5159 0.5077 0.488  0.5188\n",
            " 0.5145 0.5181 0.4691 0.4908 0.5025 0.5494 0.5534 0.556  0.5558 0.5581\n",
            " 0.5565 0.5596 0.5614 0.5585 0.5618 0.5608 0.5624 0.563  0.5632 0.5647]\n",
            "Epoch 0 | Time Spent: 14.71 Train_Loss: tensor(2.3020, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.1013\n",
            "Epoch 1 | Time Spent: 15.26 Train_Loss: tensor(2.3046, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.105\n",
            "Epoch 2 | Time Spent: 15.08 Train_Loss: tensor(2.2983, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.1173\n",
            "Epoch 3 | Time Spent: 14.87 Train_Loss: tensor(2.2877, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.1276\n",
            "Epoch 4 | Time Spent: 14.76 Train_Loss: tensor(2.3028, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.1651\n",
            "Epoch 5 | Time Spent: 14.92 Train_Loss: tensor(2.3015, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.165\n",
            "Epoch 6 | Time Spent: 14.74 Train_Loss: tensor(2.2909, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.1636\n",
            "Epoch 7 | Time Spent: 14.7 Train_Loss: tensor(2.2696, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.1903\n",
            "Epoch 8 | Time Spent: 14.59 Train_Loss: tensor(2.3232, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.1984\n",
            "Epoch 9 | Time Spent: 14.8 Train_Loss: tensor(2.2621, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.2303\n",
            "Epoch 10 | Time Spent: 14.48 Train_Loss: tensor(2.2538, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.2646\n",
            "Epoch 11 | Time Spent: 15.63 Train_Loss: tensor(2.2510, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.2701\n",
            "Epoch 12 | Time Spent: 14.96 Train_Loss: tensor(2.2245, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.2896\n",
            "Epoch 13 | Time Spent: 15.58 Train_Loss: tensor(2.2378, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.2695\n",
            "Epoch 14 | Time Spent: 15.98 Train_Loss: tensor(2.2889, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.2979\n",
            "Epoch 15 | Time Spent: 15.9 Train_Loss: tensor(2.2375, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.3003\n",
            "Epoch 16 | Time Spent: 17.25 Train_Loss: tensor(2.2654, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.3208\n",
            "Epoch 17 | Time Spent: 13.94 Train_Loss: tensor(2.2433, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.314\n",
            "Epoch 18 | Time Spent: 14.18 Train_Loss: tensor(2.1206, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.3368\n",
            "Epoch 19 | Time Spent: 14.17 Train_Loss: tensor(2.2665, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.26\n",
            "Epoch 20 | Time Spent: 14.3 Train_Loss: tensor(2.2099, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.3354\n",
            "Epoch 21 | Time Spent: 14.99 Train_Loss: tensor(2.1994, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.3448\n",
            "Epoch 22 | Time Spent: 15.2 Train_Loss: tensor(2.2015, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.3732\n",
            "Epoch 23 | Time Spent: 14.84 Train_Loss: tensor(2.1190, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.3716\n",
            "Epoch 24 | Time Spent: 14.88 Train_Loss: tensor(2.1265, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.3637\n",
            "Epoch 25 | Time Spent: 15.6 Train_Loss: tensor(2.1410, device='cuda:0', grad_fn=<AddBackward0>) Test_Accuracy1: 0.3822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "50vFTIoXoqXx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}