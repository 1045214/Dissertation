{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrHqDSyYFDFcVXVHACVsuv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mhobo/SimNets/blob/main/Jupyter_RobustnessCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvB-oIIqK9-w"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "import random as rnd\n",
        "from scipy.stats import bernoulli\n",
        "import time\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0., std=1.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "        \n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
      ],
      "metadata": {
        "id": "laS_D1AeLdrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_workers = 8\n",
        "Changes = [transforms.GaussianBlur(5,5),transforms.RandomHorizontalFlip(),transforms.RandomVerticalFlip()]\n",
        "# [transforms.Pad(4), transforms.RandomCrop(28)], problem cos list\n",
        "# transforms.RandomHorizontalFlip(),transforms.RandomVerticalFlip(),"
      ],
      "metadata": {
        "id": "WG7M1m8HLdtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_rob_transform = transforms.Compose([\n",
        "    Changes[i], # (3,3) Gives like a 3% drop, (5,5) like an 8% drop\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "#transforms.RandomHorizontalFlip(),\n",
        "#transforms.RandomVerticalFlip()\n",
        "# transforms.Pad(4), transforms.RandomCrop(28)\n",
        "# transforms.GaussianBlur(5,5)\n",
        "# AddGaussianNoise(0., 1.)\n",
        "\n",
        "# FashionMNIST dataset\n",
        "# Dataset objects and Dataloader objects are objects Python defines\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='data/',\n",
        "                                             train=True, \n",
        "                                             transform=transforms.ToTensor(),  #replace with = transform after\n",
        "                                             download=True)\n",
        "\n",
        "test_rob_dataset = torchvision.datasets.CIFAR10(root='data/',\n",
        "                                            train=False,#need comma here to get transforms back\n",
        "                                            transform=test_rob_transform)\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                            batch_size= batch_size, \n",
        "                                            shuffle=True, num_workers = num_workers)\n",
        "\n",
        "\n",
        "test_rob_loader = torch.utils.data.DataLoader(dataset=test_rob_dataset,\n",
        "                                            batch_size=128, \n",
        "                                            shuffle=False, num_workers = num_workers)"
      ],
      "metadata": {
        "id": "-P6VQ2v9Ldvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(P_DML_ss = 0,P_DML = 1, P_SS =1, ss = False):  \n",
        "  net.train()\n",
        "  net2.train()\n",
        "\n",
        "\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    \n",
        "    images_flat = torch.flatten(images.to(device),start_dim = 1) # Will want to flatten this in  my case\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    guesses, _ = net(images_flat)\n",
        "    loss = loss_fn(guesses,labels)\n",
        "\n",
        "    guesses2, _ = net2(images_flat)\n",
        "    loss2 = loss_fn(guesses2,labels)\n",
        "    \n",
        "    #DML_Loss\n",
        "    loss_DML = P_DML*loss_fn(guesses,guesses2.softmax(dim=1).detach())\n",
        "    loss += loss_DML\n",
        "\n",
        "    loss_DML2 = P_DML*loss_fn(guesses2,guesses.softmax(dim=1).detach())\n",
        "    loss2 += loss_DML2\n",
        "\n",
        "    if ss == True: # Rotation Predictions\n",
        "      bx = images_flat\n",
        "      \n",
        "      curr_batch_size = bx.size(0)\n",
        "      by_prime = torch.cat((torch.zeros(bx.size(0)), torch.ones(bx.size(0)),\n",
        "                                  2*torch.ones(bx.size(0)), 3*torch.ones(bx.size(0))), 0).long()\n",
        "      \n",
        "      bx = images\n",
        "      bx = torch.cat((bx, torch.rot90(bx,1,[2,3]),torch.rot90(bx, 2,[2,3]), torch.rot90(bx, 3,[2,3])), 0)\n",
        "      bx = torch.flatten(bx,start_dim=1)\n",
        "      \n",
        "      bx, by_prime = bx.to(device), by_prime.to(device)\n",
        "\n",
        "      _, pen = net(bx)\n",
        "      _, pen2 = net2(bx)\n",
        "\n",
        "      ss_guesses = net.rot_pred(pen)\n",
        "      ss_guesses2 = net2.rot_pred(pen2)\n",
        "\n",
        "      ss_loss = P_SS*loss_fn(ss_guesses, by_prime)\n",
        "      ss_loss2 = P_SS*loss_fn(ss_guesses2, by_prime)\n",
        "\n",
        "      loss_DML_ss = P_DML_ss*loss_fn(ss_guesses,ss_guesses2.softmax(dim=1).detach())\n",
        "      loss += loss_DML_ss\n",
        "\n",
        "      loss_DML2_ss = P_DML_ss*loss_fn(ss_guesses2,ss_guesses.softmax(dim=1).detach())\n",
        "      loss2 += loss_DML2_ss\n",
        "\n",
        "\n",
        "      loss += ss_loss\n",
        "      loss2 += ss_loss2\n",
        "\n",
        "    optimizer.zero_grad() \n",
        "    \n",
        "    loss.backward()\n",
        "    loss2.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  scheduler.step(loss)\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "x-3UHM2jLdxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(test_loader = test_rob_loader):\n",
        "  net.eval()\n",
        "  net2.eval()\n",
        "  with torch.no_grad():\n",
        "      correct = 0\n",
        "      total = 0\n",
        "\n",
        "      correct2 = 0\n",
        "      total2 = 0      \n",
        "\n",
        "      for images, labels in test_loader:\n",
        "        images = torch.flatten(images.to(device),start_dim = 1)\n",
        "        labels = labels.to(device)\n",
        "        guesses,_ = net(images)\n",
        "        _, predicted = torch.max(guesses.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        test_accuracy = correct / total\n",
        "\n",
        "        guesses2,_ = net2(images)\n",
        "        _, predicted2 = torch.max(guesses2.data, 1)\n",
        "        total2 += labels.size(0)\n",
        "        correct2 += (predicted2 == labels).sum().item()\n",
        "  return test_accuracy"
      ],
      "metadata": {
        "id": "QfmadG5RLkZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the layers we want in between, need somewhere to check dimension of dataset?\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self,layer_widths): \n",
        "# Alternatively \n",
        "    super().__init__()\n",
        "    self.m = len(layer_widths) #self stores it as characteristic of this NN\n",
        "    self.layers = [] # Empty list to store layers\n",
        "    self.non_lin = nn.ReLU() # nn.ReLU isn't the function, but the function that constructs a relu (by saying =)\n",
        "    for i in range(self.m-1): #does this make m-1 iterations?\n",
        "      self.layers.append(nn.Linear(layer_widths[i], layer_widths[i+1])) # Need matrix maths to work, so have (a,b) thenn (b,c)\n",
        "      self.add_module(\"layer\"+str(i), self.layers[-1])\n",
        " \n",
        "  def forward(self, x):\n",
        "    z = x\n",
        "    for i in range(self.m - 1):\n",
        "      y = z\n",
        "      z = self.non_lin(self.layers[i](z))\n",
        "    return z,y\n",
        "# Probably better off hardcoding, sanity check and because the layers are designed with the dataset in mind"
      ],
      "metadata": {
        "id": "hJizNxvPLzTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:1')\n",
        "num_epochs = 50\n",
        "\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "gFtegCttLzWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining loss and optimiser\n",
        "torch.manual_seed(0)\n",
        "m = 3\n",
        "Inside = [50]*m\n",
        "Inside.insert(0,(28*28))\n",
        "Inside.append(10)\n",
        "Inside = [(3*32*32),1536,768,384,192,96,10] # [128,10] good, [128,64, 32,10]] also gets to 87% in 15 epochs, SSL and DML drop 10% accuracy for uncorrupted data for first class at that size\n",
        "# For 0.5 corruption SS does a lot better, 55% for vanilla and DML, 75% for SS\n",
        "# 32,10 suggested here: https://www.kaggle.com/code/pavansanagapati/a-simple-cnn-model-beginner-guide\n",
        "net_safe = MLP(Inside)\n",
        "net2_safe = MLP(Inside)\n",
        "\n",
        "net_safe.rot_pred  = nn.Linear(96, 4)\n",
        "net2_safe.rot_pred = nn.Linear(96, 4)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss() # Also an nn.module"
      ],
      "metadata": {
        "id": "8FuyrmIEMIGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DML = [0,1,0,1,0]\n",
        "SSL = [0,0,1,1,1]\n",
        "DML_SSL = [0,0,0,0,1]\n",
        "Type = ['Unchanged','DML','SSL','Out','In']"
      ],
      "metadata": {
        "id": "W44lRRktMIWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "All_accs = []\n",
        "for k in range(3):\n",
        "    for i in range(3): # types of robustness\n",
        "        \n",
        "        test_rob_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                        transforms.ToTensor()])\n",
        "            \n",
        "        test_rob_dataset = torchvision.datasets.CIFAR10(root='data/',\n",
        "                                            train=False,#need comma here to get transforms back\n",
        "                                            transform=test_rob_transform)    \n",
        "                \n",
        "                \n",
        "        test_rob_loader = torch.utils.data.DataLoader(dataset=test_rob_dataset,\n",
        "                                            batch_size=128, \n",
        "                                            shuffle=False, num_workers = num_workers)\n",
        "            \n",
        "        for t in range(5):\n",
        "            j = t+2\n",
        "            \n",
        "            \n",
        "            net = copy.deepcopy(net_safe)\n",
        "            net2 = copy.deepcopy(net2_safe)\n",
        "            \n",
        "            net.to(device)\n",
        "            net2.to(device)\n",
        "            \n",
        "            \n",
        "            \n",
        "            optimizer = torch.optim.SGD(list(net.parameters()) +list(net2.parameters()), lr= learning_rate) # An optimiser object\n",
        "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 3)\n",
        "              \n",
        "            loss = np.array([])\n",
        "            acc = np.array([])\n",
        "              \n",
        "            for epoch in range(num_epochs):\n",
        "                  \n",
        "                begin_epoch = time.time()\\\n",
        "                  \n",
        "                new_train_loss = train(P_DML_ss= DML_SSL[j], P_DML = DML[j], P_SS = SSL[j],ss = True)\n",
        "                \n",
        "                \n",
        "                loss = np.append(loss,new_train_loss.cpu().detach().numpy())\n",
        "                new_acc = test(test_rob_loader)\n",
        "                acc = np.append(acc, new_acc)\n",
        "              \n",
        "                #print('Epoch', epoch, '| Time Spent:', round(time.time() - begin_epoch, 2), 'Train_Loss:',new_train_loss, 'Test_Accuracy1:', new_acc)\n",
        "            print('Random Horizontal FLip')\n",
        "            print(Type[j])\n",
        "            print(acc)\n",
        "            All_accs.append((acc,Type[j]))\n",
        "            \n",
        "with open('Rob.pickle', 'wb') as handle: \n",
        "    pickle.dump(All_accs, handle)"
      ],
      "metadata": {
        "id": "OXoqphshMBzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E03-iLo2MOoj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}